# Assignment Scraper - Environment Configuration
# Copy this file to .env and customize the values for your environment

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
APP_NAME=AssignmentScraper
APP_VERSION=1.0.0
DEBUG=false
LOG_LEVEL=INFO

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# PostgreSQL connection string
DATABASE_URL=postgresql+asyncpg://scraper:scraper123@localhost:5432/scraper_db

# Database connection pool settings
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=30

# =============================================================================
# REDIS CONFIGURATION
# =============================================================================
# Redis connection for caching
REDIS_URL=redis://localhost:6379/0

# Cache TTL in seconds (3600 = 1 hour)
REDIS_CACHE_TTL=3600

# =============================================================================
# CELERY CONFIGURATION (Background Tasks)
# =============================================================================
# Celery broker (message queue)
CELERY_BROKER_URL=redis://localhost:6379/1

# Celery result backend (task results storage)
CELERY_RESULT_BACKEND=redis://redis:6379/2

# =============================================================================
# SECURITY SETTINGS
# =============================================================================
# IMPORTANT: Change this to a secure random string in production!
SECRET_KEY=your-super-secret-key-change-this-in-production-please

# JWT algorithm for token encoding
ALGORITHM=HS256

# Token expiration time in minutes
ACCESS_TOKEN_EXPIRE_MINUTES=30

# =============================================================================
# WEB SCRAPING CONFIGURATION
# =============================================================================
# Default timeout for HTTP requests (seconds)
SCRAPING_TIMEOUT=30

# Maximum number of retry attempts
SCRAPING_MAX_RETRIES=3

# Delay between requests to be respectful (seconds)
SCRAPING_DELAY=1.0

# Maximum concurrent scraping operations
SCRAPING_MAX_CONCURRENT=5

# Default User Agent string for HTTP requests
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36

# =============================================================================
# RATE LIMITING
# =============================================================================
# Maximum requests per minute per client
RATE_LIMIT_PER_MINUTE=60

# Burst allowance for rate limiting
RATE_LIMIT_BURST=10

# =============================================================================
# BROWSER AUTOMATION SETTINGS
# =============================================================================
# Run browsers in headless mode (true for production)
BROWSER_HEADLESS=true

# Browser operation timeout (milliseconds)
BROWSER_TIMEOUT=30000

# =============================================================================
# MONITORING & METRICS
# =============================================================================
# Port for Prometheus metrics endpoint
PROMETHEUS_PORT=8001

# Enable metrics collection
ENABLE_METRICS=true

# =============================================================================
# API CONFIGURATION
# =============================================================================
# API version prefix
API_V1_STR=/api/v1

# CORS allowed origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:8080,http://localhost:8000

# =============================================================================
# PRODUCTION OVERRIDES
# =============================================================================
# When deploying to production, override these values:
#
# DEBUG=false
# LOG_LEVEL=WARNING
# SECRET_KEY=<generate-a-secure-random-key>
# DATABASE_URL=<your-production-database-url>
# REDIS_URL=<your-production-redis-url>
# CELERY_BROKER_URL=<your-production-message-broker>
# BROWSER_HEADLESS=true
# SCRAPING_DELAY=2.0
# RATE_LIMIT_PER_MINUTE=30

# =============================================================================
# DOCKER COMPOSE OVERRIDES
# =============================================================================
# When using docker-compose, these are automatically set:
#
# DATABASE_URL=postgresql+asyncpg://scraper:scraper123@postgres:5432/scraper_db
# REDIS_URL=redis://redis:6379/0
# CELERY_BROKER_URL=redis://redis:6379/1
# CELERY_RESULT_BACKEND=redis://redis:6379/2
